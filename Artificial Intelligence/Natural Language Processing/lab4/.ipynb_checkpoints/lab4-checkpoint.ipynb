{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52815b5d",
   "metadata": {},
   "source": [
    "# AIC-5102B  Lab4 : text classification, machine translation\n",
    "\n",
    "This lab must be done on mvxr.esiee.fr -> please visit https://mvproxy.esiee.fr to see the connection procedure\n",
    "\n",
    "## Work to do and assessment policy:\n",
    "\n",
    "- The two parts of this lab are completely independent.\n",
    "- You are only requested to do part A to fully validate your grade\n",
    "- Part B comes as bonus points, as your mark will be computed as \n",
    "$$\n",
    "mark = \\min(20, part_A + \\frac{1}{2} part_B)\n",
    "$$\n",
    "- Simply fill this notebook and drop it on mvproxy no later than november 20th 23:59\n",
    "\n",
    "\n",
    "## Part A : text classification\n",
    "\n",
    "In this part, you will have to finish the implementations of two RNN-based models shown on slides 28 and 38 of [Chapter 4](https://perso.esiee.fr/~hilairex/AIC-5102B/rnn.pdf). Both networks accept words as input, from sentences which don't exceed a certain length, and aim to perform text classification. \n",
    "\n",
    "You will work on the the IMDB reviews dataset, hosted by Kaggle [here](https://www.kaggle.com/code/trentpark/data-analysis-basics-imdb-dataset). The reviews have two outcomes : positive, or negative. A copy of this dataset can be found locally in /home/shared.\n",
    "\n",
    "The following code snippets perform the first steps on text for you - loading, vectorising, and training a basic (non-recurrent) FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec980dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 14:32:01.920436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "reviews = pd.read_csv(\"/home/shared/IMDB Dataset.csv\")\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed6c50",
   "metadata": {},
   "source": [
    "We first perform a standard test/train split. During development, I strongly suggest that you first use a small amount of samples (1000) for validation. IMDb has 50000 reviews, which is too much. Keep in ming that training RNNs is *slow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "18e7f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews['review']\n",
    "test = reviews['sentiment']\n",
    "test = LabelEncoder().fit_transform(test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, test, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55439f5d",
   "metadata": {},
   "source": [
    "The next step is to vectorize the text. In Lab3, I provided a vecto() function which did this, with relevant padding. I also mentioned Keras offered a TextVectorization layer which did exactly the same job. Its effects are shown below. \n",
    "\n",
    "In particular, note that unknown words yield an index of 1, and 0 is used for padding. So real indexation starts at index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c57ea20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=int64, numpy=\n",
       "array([[ 8, 10,  2,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  7,  1,  0,  0,  0,  0,  0,  0]])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text vectorization : quick demo\n",
    "vecto= tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=99, output_mode='int', output_sequence_length=10)\n",
    "vecto.adapt([[\"I am the king of the world\"],[\"You are the queen\"]])\n",
    "vecto([[\"I am the queen\"],[\"World is king unknown\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef63e2e",
   "metadata": {},
   "source": [
    "We now change the call to adapt the layer to our train data. Note that IMDb reviews are rather long (about 300 words / review on average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61771054",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words=3000  # the vocabulary size\n",
    "seq_len=300     # maximum sequence length\n",
    "vecto= tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_words, output_mode='int', output_sequence_length=300)\n",
    "vecto.adapt(train['review'].to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5500254",
   "metadata": {},
   "source": [
    "We are now ready to define our model. Below, I first demonstrate a model with input and vectorization layer alone ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850f1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 244ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  10,  203,    2, 1049,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building model : vectorization alone\n",
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.predict(['I am the king'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c5a63",
   "metadata": {},
   "source": [
    "As we saw in labs 2 and 3, embeddings are mandatory. Hence, we will add an Embedding layer, but as opposed as what we did before, we will not initialize if from LSA, nor put it constant. Instead, we will let the model optimize this layer, possibly using dropout (if you use the related option). \n",
    "The dimension of 80 below is a crude estimation (barely from lab2 and results on LSA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4f07ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 80)           240160    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 240160 (938.12 KB)\n",
      "Trainable params: 240160 (938.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 205ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00565327, -0.03459281,  0.01273665, ..., -0.03983495,\n",
       "         -0.03929449,  0.00613182],\n",
       "        [-0.00689242,  0.00185695,  0.0247079 , ...,  0.01098173,\n",
       "         -0.02924296,  0.01897011],\n",
       "        [ 0.01561158,  0.01476458, -0.00319093, ...,  0.0498675 ,\n",
       "          0.03967917,  0.02086109],\n",
       "        ...,\n",
       "        [-0.04816231, -0.02181544,  0.04463018, ...,  0.04527048,\n",
       "         -0.0108258 ,  0.01364191],\n",
       "        [-0.04816231, -0.02181544,  0.04463018, ...,  0.04527048,\n",
       "         -0.0108258 ,  0.01364191],\n",
       "        [-0.04816231, -0.02181544,  0.04463018, ...,  0.04527048,\n",
       "         -0.0108258 ,  0.01364191]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.predict(['I am the king'])\n",
    "# TODO Implement RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc3488",
   "metadata": {},
   "source": [
    "Now it's up to you to devise and train two models which conforms those shown on slides 28 and 38 of Chapter 4 [here](https://perso.esiee.fr/~hilairex/AIC-5102B/rnn.pdf). Some pieces of advice :\n",
    "- Try first to reproduce the one on slide 28 using a SimpleRNN or LSTM. That one is the simplest.\n",
    "- Both have a return_sequence option, beware to what you are computing !\n",
    "- Remember that embedding turn integer indexes into vectors. Hence your input data is a sequence of *vectors* whatever type of RNN you use. Be careful to dimensionality and shapes.\n",
    "- In the end, you want a single scalar to represent a decision : yes or no (positive or negative)\n",
    "- Once training is done, you may try a predict() on thetest data, but such kind of simple (non stacked) RNN achieves an accuracy of about 82% at best (see Kaggle's benchmarks). \n",
    "- Keras has a [Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/) and a [Concatenate](https://keras.io/api/layers/merging_layers/concatenate/) layers, which can be very handy. You may however build your model without using them, by using variables to connect the output(s) of a layer to the input of a new one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f797c87",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "<h3>Text classification with forward RNN</h3>\n",
    "<h4>Model definition</h4><br/>\n",
    "For this neural netword, the LSTM layer fit pretty well. It must have 128 cells since it is appropriate to use a power of two number of cells and that the previous embedding layer returns an output of size 80. Also, the loss function has been changed to a binary_crossentropy because it fits better to binomial classification. Thus, it is necessary to change the activation function of the output layer from softmax (designed for multinomial classification) to sigmoid.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80f3f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_59 (Embedding)    (None, 300, 80)           240160    \n",
      "                                                                 \n",
      " lstm_65 (LSTM)              (None, 300, 128)          107008    \n",
      "                                                                 \n",
      " global_max_pooling1d_17 (G  (None, 128)               0         \n",
      " lobalMaxPooling1D)                                              \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 351329 (1.34 MB)\n",
      "Trainable params: 351329 (1.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing layers\n",
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "# RNN\n",
    "model.add(tf.keras.layers.LSTM(128, activation='sigmoid', return_sequences=True))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "# Classification\n",
    "model.add(tf.keras.layers.Dense(32, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28ec63",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "The compilation give satisfying results since there is no error. Also, we notice that each output's dimension fits the following input's constraints.<br/><h4>Training the model</h4><br/>To test the perks of this model, we can train it over a training dataset.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b09fbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 756s 301ms/step - loss: 0.4503 - accuracy: 0.7804\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 921s 368ms/step - loss: 0.3087 - accuracy: 0.8681\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 762s 305ms/step - loss: 0.2612 - accuracy: 0.8932\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 755s 302ms/step - loss: 0.2313 - accuracy: 0.9086\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 767s 307ms/step - loss: 0.2063 - accuracy: 0.9196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fdb55c88190>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ecf47",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "The training process nearly took an hour. It achieves very good scores, which are over the expected results from Kaggle. Kaggle expects a <b>82%</b> accuracy where we obtain <b>91%</b>. We can suppose that it is due to some overfitting. To verify this hypothesis, it is possible to score the model over a test dataset and check if there is a gap between the training score and the testing score.\n",
    "<br/><h4>Evaluating the model</h4>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ffe96214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 72s 226ms/step - loss: 0.3158 - accuracy: 0.8711\n",
      "Test loss: 0.3158118426799774\n",
      "Test accuracy: 0.8711000084877014\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecccdc3",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "For the test dataset, we obtain a greater loss (<b>0.31</b>) and a smaller accuracy (<b>87%</b>). It testifies of some overfitting. To fix this issue, it is possible to add some dropout layers inside of the neural network. However, to fit to the course model, we made the choice of not implementing this feature and to keep the small overfitting.  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffab03",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "<h3>Text classification with bidirectional RNN</h3>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5944363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_63 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization_1 (Text  (None, 300)                  0         ['input_63[0][0]']            \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_60 (Embedding)    (None, 300, 80)              240160    ['text_vectorization_1[62][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " lstm_66 (LSTM)              (None, 128)                  107008    ['embedding_60[0][0]']        \n",
      "                                                                                                  \n",
      " lstm_67 (LSTM)              (None, 128)                  107008    ['embedding_60[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat_9 (TFOpLambda)    (None, 256)                  0         ['lstm_66[0][0]',             \n",
      "                                                                     'lstm_67[0][0]']             \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 64)                   16448     ['tf.concat_9[0][0]']         \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 1)                    65        ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 470689 (1.80 MB)\n",
      "Trainable params: 470689 (1.80 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing layers\n",
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "# Bidirectional layers\n",
    "FLSTM = tf.keras.layers.LSTM(128, activation='sigmoid')(model.layers[1].output)\n",
    "BLSTM = tf.keras.layers.LSTM(128, activation='sigmoid', go_backwards=True)(model.layers[1].output)\n",
    "concatenated = tf.concat([FLSTM, BLSTM], axis=-1)\n",
    "# Classification layers\n",
    "hidden_dense = tf.keras.layers.Dense(64, activation='sigmoid')(concatenated)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_dense)\n",
    "new_model = tf.keras.Model(inputs=model.input, outputs=output)\n",
    "\n",
    "new_model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4380045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 912s 362ms/step - loss: 0.4886 - accuracy: 0.7438\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 901s 360ms/step - loss: 0.3001 - accuracy: 0.8758\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 900s 360ms/step - loss: 0.2720 - accuracy: 0.8901\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 901s 360ms/step - loss: 0.2470 - accuracy: 0.8997\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 900s 360ms/step - loss: 0.2272 - accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fdab41ff290>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(X_train, y_train, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ff8a5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 74s 232ms/step - loss: 0.3051 - accuracy: 0.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30506381392478943, 0.8745999932289124]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0814b90",
   "metadata": {},
   "source": [
    "## Part B : inference in neural machine translation\n",
    "\n",
    "In this part, you will have to write a piece of code which will mimic the beam decoding algorithm shown on slides 30+ of [Chapter 5](https://perso.esiee.fr/~hilairex/AIC-5102B/lstm.pdf)\n",
    "\n",
    "The following code implements the network shown on slide 26, with the difference that inputs will not be words, but characters - this drastically reduces the memory requirements, to the price of a lower accuracy, however.\n",
    "\n",
    "The dataset is derived from transcripts of the European parliament - see https://www.statmt.org/europarl/\n",
    "We will translate english sentences to french. We first load and sample the transcripts from local files. Note that the '\\</s\\>' special word on slide 26 has been replaced by a '\\x03' character to denote the end of a sentence. Likewise, the beginning of a sentence (which is missing in the decoder part, as it needs an input word or character) will be a '\\x02' special character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18905e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.statmt.org/europarl/\n",
    "\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# data processing\n",
    "english=open('/home/shared/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "french=open('/home/shared/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "# begin and end special characters\n",
    "begin='\\x02'\n",
    "end='\\x03'\n",
    "\n",
    "tran=[]\n",
    "i=0\n",
    "for x,y in zip(english,french):\n",
    "    if (len(x) > 0) and (len(x) < 30) and (len(y) > 0) and (len(y) < 40):\n",
    "        tran.append((x+end,begin+y+end))\n",
    "        i=i+1\n",
    "        \n",
    "\n",
    "# without sampling the above produces about 60k samples -> too much\n",
    "tran,_=train_test_split(tran,train_size=20000)\n",
    "nsamples=len(tran) # about 60k samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "103c4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Thank you very much, Mr Blak.\\x03', '\\x02Je vous remercie, Monsieur Blak.\\x03')\n"
     ]
    }
   ],
   "source": [
    "print(tran[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368f6f7",
   "metadata": {},
   "source": [
    "We then build the vocabularies (=set of chars), and char->ord and ord->char dictionaries, for source (index=0) and target (index=1) languages. Those will be useful when vectorising sentences . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1318bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc=[]\n",
    "char2num=[]\n",
    "num2char=[]\n",
    "maxlen=[]\n",
    "\n",
    "for lang in range(0,2):\n",
    "    voc.append(sorted(set([c for w in tran for c in w[lang]])))\n",
    "    c2n={}\n",
    "    n2c={}\n",
    "    for i in range(0,len(voc[lang])):\n",
    "        n2c[i]=voc[lang][i]\n",
    "        c2n[voc[lang][i]]=i\n",
    "    char2num.append(c2n)\n",
    "    num2char.append(n2c)\n",
    "    maxlen.append(max([len(w[lang]) for w in tran]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4a763",
   "metadata": {},
   "source": [
    "Next comes vectorisation : we replace every character directly by its one-hot binary representation. As a result, the vectorisation of a sentence is directly a tensor, and not a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c3e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation of sentences\n",
    "en=0\n",
    "fr=1\n",
    "    \n",
    "vecto=[]\n",
    "for lang in range(0,2):\n",
    "    vec=np.zeros((nsamples,maxlen[lang],len(voc[lang])), dtype='float32')\n",
    "    for sample in range(0,nsamples):\n",
    "        for row in range(0,len(tran[sample][lang])):\n",
    "            vec[sample,row,char2num[lang][tran[sample][lang][row]]]=1\n",
    "    vecto.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774bf60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Petitions: see Minutes\\x03', '\\x02Pétitions: voir procès-verbal\\x03')\n"
     ]
    }
   ],
   "source": [
    "print(tran[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6dc60",
   "metadata": {},
   "source": [
    "Finally comes the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f7f8ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"en2fr128\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)       [(None, None, 131)]          0         []                            \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)       [(None, None, 144)]          0         []                            \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)               [(None, 128),                133120    ['input_33[0][0]']            \n",
      "                              (None, 128),                                                        \n",
      "                              (None, 128)]                                                        \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)               [(None, None, 128),          139776    ['input_34[0][0]',            \n",
      "                              (None, 128),                           'lstm_8[0][1]',              \n",
      "                              (None, 128)]                           'lstm_8[0][2]']              \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, None, 144)            18576     ['lstm_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 291472 (1.11 MB)\n",
      "Trainable params: 291472 (1.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "\n",
    "# number of units to use in LSTM layers\n",
    "lstm_units=128\n",
    "\n",
    "# encoder side\n",
    "# input data = any string of the source language\n",
    "enc_input = keras.layers.Input(shape=(None, len(voc[0])))\n",
    "\n",
    "# transform this string by an LSTM layer\n",
    "[enc_out, enc_hidden, enc_cell] = keras.layers.LSTM(units=lstm_units, return_state=True)(enc_input)\n",
    "\n",
    "# decoder side\n",
    "# input is a translated string in the target language\n",
    "dec_input = keras.layers.Input(shape=(None,len(voc[1])))\n",
    "\n",
    "# the LSTM layer must return two vectors : the hidden state vector, and the cell vector\n",
    "# Must also return the full sequence, as the decoder is trained in teacher forcing mode\n",
    "dec_lstm = keras.layers.LSTM(units=128, return_state=True, return_sequences=True)\n",
    "[dec_out,dec_hidden,dec_cell] = dec_lstm(dec_input, initial_state=[enc_hidden,enc_cell])\n",
    "dec_output = keras.layers.Dense(units=len(voc[1]), activation='softmax', use_bias=True)(dec_out)\n",
    "\n",
    "# final model\n",
    "model= keras.Model(inputs=[enc_input, dec_input], outputs=dec_output, name='en2fr'+str(lstm_units))\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94b543",
   "metadata": {},
   "source": [
    "The following snippet offers to train or load pretrained model from disk. Do *always* load a model from disk, on my HP380 server, training takes *hours* of computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b587a077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 47s 164ms/step - loss: 1.9703 - val_loss: 1.9086\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 37s 155ms/step - loss: 1.8813 - val_loss: 1.8926\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 36s 152ms/step - loss: 1.8736 - val_loss: 1.8870\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 36s 152ms/step - loss: 1.8697 - val_loss: 1.8823\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 36s 152ms/step - loss: 1.8667 - val_loss: 1.8830\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 36s 152ms/step - loss: 1.8642 - val_loss: 1.8803\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8634 - val_loss: 1.8810\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 36s 155ms/step - loss: 1.8626 - val_loss: 1.8778\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 36s 152ms/step - loss: 1.8604 - val_loss: 1.8740\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 36s 152ms/step - loss: 1.8598 - val_loss: 1.8765\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8596 - val_loss: 1.8839\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8598 - val_loss: 1.8764\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8592 - val_loss: 1.8746\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8587 - val_loss: 1.8762\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8586 - val_loss: 1.8769\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8586 - val_loss: 1.8780\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8588 - val_loss: 1.8776\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8587 - val_loss: 1.8778\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8582 - val_loss: 1.8801\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8576 - val_loss: 1.8798\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8580 - val_loss: 1.8779\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8568 - val_loss: 1.8759\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 36s 155ms/step - loss: 1.8568 - val_loss: 1.8810\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 36s 155ms/step - loss: 1.8567 - val_loss: 1.8717\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8552 - val_loss: 1.8732\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8560 - val_loss: 1.8741\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8564 - val_loss: 1.8765\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8563 - val_loss: 1.8736\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8568 - val_loss: 1.8710\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8565 - val_loss: 1.8725\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8566 - val_loss: 1.8696\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8557 - val_loss: 1.8732\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8560 - val_loss: 1.8732\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8556 - val_loss: 1.8720\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8563 - val_loss: 1.8801\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8565 - val_loss: 1.8734\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8562 - val_loss: 1.8799\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8565 - val_loss: 1.8760\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8566 - val_loss: 1.8698\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8556 - val_loss: 1.8729\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8558 - val_loss: 1.8728\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8562 - val_loss: 1.8777\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8573 - val_loss: 1.8779\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8573 - val_loss: 1.8710\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8553 - val_loss: 1.8720\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8555 - val_loss: 1.8739\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8560 - val_loss: 1.8792\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8570 - val_loss: 1.8738\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8573 - val_loss: 1.8779\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8583 - val_loss: 1.8739\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8567 - val_loss: 1.8796\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8563 - val_loss: 1.8769\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8572 - val_loss: 1.8782\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8566 - val_loss: 1.8768\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8566 - val_loss: 1.8744\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8561 - val_loss: 1.8703\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8573 - val_loss: 1.8791\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8568 - val_loss: 1.8814\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8570 - val_loss: 1.8718\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8567 - val_loss: 1.8778\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8563 - val_loss: 1.8684\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8554 - val_loss: 1.8720\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8567 - val_loss: 1.8746\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8571 - val_loss: 1.8762\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8567 - val_loss: 1.8731\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8567 - val_loss: 1.8757\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8558 - val_loss: 1.8777\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8572 - val_loss: 1.8720\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8570 - val_loss: 1.8738\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8560 - val_loss: 1.8744\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8556 - val_loss: 1.8760\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8559 - val_loss: 1.8726\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8573 - val_loss: 1.8699\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8556 - val_loss: 1.8697\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8560 - val_loss: 1.8734\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8567 - val_loss: 1.8736\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8562 - val_loss: 1.8750\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 36s 155ms/step - loss: 1.8552 - val_loss: 1.8749\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8559 - val_loss: 1.8746\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8566 - val_loss: 1.8831\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8572 - val_loss: 1.8774\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8562 - val_loss: 1.8736\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8569 - val_loss: 1.8780\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8578 - val_loss: 1.8754\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8571 - val_loss: 1.8719\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8562 - val_loss: 1.8728\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8579 - val_loss: 1.8796\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8580 - val_loss: 1.8711\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8565 - val_loss: 1.8719\n",
      "Epoch 90/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8566 - val_loss: 1.8762\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8564 - val_loss: 1.8795\n",
      "Epoch 92/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8560 - val_loss: 1.8756\n",
      "Epoch 93/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8564 - val_loss: 1.8730\n",
      "Epoch 94/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8574 - val_loss: 1.8766\n",
      "Epoch 95/100\n",
      "235/235 [==============================] - 36s 153ms/step - loss: 1.8569 - val_loss: 1.8757\n",
      "Epoch 96/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8574 - val_loss: 1.8701\n",
      "Epoch 97/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8578 - val_loss: 1.8762\n",
      "Epoch 98/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8577 - val_loss: 1.8776\n",
      "Epoch 99/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8575 - val_loss: 1.8777\n",
      "Epoch 100/100\n",
      "235/235 [==============================] - 36s 154ms/step - loss: 1.8585 - val_loss: 1.8777\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39m[vecto[\u001b[38;5;241m0\u001b[39m],vecto[\u001b[38;5;241m1\u001b[39m]], y\u001b[38;5;241m=\u001b[39my, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      9\u001b[0m     saved_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/boiss/en2fra\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(lstm_units)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(saved_model)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(saved_model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "#saved_model='/home/shared/en2fra'+str(lstm_units)\n",
    "saved_model=''\n",
    "if saved_model == '':\n",
    "    # teacher forcing : expected output is the same than the decoded\n",
    "    # sentence, except that it is shifted one time unit forward\n",
    "    y= np.ndarray(shape=vecto[1].shape)\n",
    "    y[0:nsamples-1,:,:]= vecto[1][1:nsamples,:,:]\n",
    "    model = model.fit(x=[vecto[0],vecto[1]], y=y, validation_split=0.25, epochs=100, batch_size=64)\n",
    "    saved_model='/home/boiss/en2fra'+str(lstm_units)\n",
    "    model.save(saved_model)\n",
    "else:\n",
    "    model= keras.models.load_model(saved_model)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40988c",
   "metadata": {},
   "source": [
    "### Work to do : beam searching\n",
    "    \n",
    "Use the trained model below, including its final states, to write a piece of code which will execute a memoryless beam searching algorithm. This should do the following:\n",
    "1. Given an input string, encode it using the encoder model. That will give you a final hidden state (enc_hidden) and cell state (enc_cell)\n",
    "2. Set (enc_hidden,enc_cell) as the initial states of a decoder model, which should behave exactly as the one you built in the \"decoder side\" section, except that it has an initial state that must be set for any new input string\n",
    "3. Set the current character to '\\x02', to initially denote the beginning of the translated sentence \n",
    "4. If you feed the (vectorised) current character to the decoder, and ask for its prediction, you will obtain a probability distribution\n",
    "4. Following beam searching, from this probability distribution you should normally extract the $n$ most probable characters. We will simplify and choose $n=1$ (memoryless beam search) to keep the best candidate\n",
    "5. Add this best candidate to your decoded string, set the current character to this character, and loop to step 3 unless the decoded sentence is too long ($length > len(voc[1])$) or an '\\x03' character is predicted (end of sentence)\n",
    "\n",
    "Simply let your code produce its results. Don't expect good outputs, even though the model is properly built, there are issues with the data preparation, as explained in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8cf64f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 973ms/step\n"
     ]
    }
   ],
   "source": [
    "# define the encoder model\n",
    "encoder_model = keras.Model(inputs=enc_input, outputs=[enc_out, enc_hidden, enc_cell])\n",
    "# define the input string\n",
    "input_string = 'Hello world'+end\n",
    "\n",
    "# Vectorize the input string\n",
    "input_sequence = np.zeros((1, maxlen[0], len(voc[0])), dtype='float32')\n",
    "for i in range(len(input_string)):\n",
    "    input_sequence[0, i, char2num[0][input_string[i]]] = 1.\n",
    "    \n",
    "out, hidden, cell = encoder_model.predict(input_sequence)\n",
    "\n",
    "# set the initial states of the decoder\n",
    "states_value = [hidden, cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "edd6fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layers for any new strings\n",
    "dec_hidden_input = keras.layers.Input(shape=(lstm_units,))\n",
    "dec_cell_input = keras.layers.Input(shape=(lstm_units,))\n",
    "dec_initial_states = [dec_hidden_input, dec_cell_input]\n",
    "[dec_output, dec_hidden, dec_cell] = dec_lstm(dec_input, initial_state=dec_initial_states)\n",
    "dec_output = keras.layers.Dense(units=len(voc[1]), activation='softmax', use_bias=True)(dec_output)\n",
    "dec_model = keras.Model(inputs=[dec_input] + dec_initial_states, outputs=[dec_output, dec_hidden, dec_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8ee62b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ybbžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžžž\n"
     ]
    }
   ],
   "source": [
    "current_char = begin\n",
    "current_hidden = hidden\n",
    "current_cell = cell\n",
    "dec_sentence = ''\n",
    "\n",
    "while current_char != end and len(dec_sentence) < maxlen[1]:\n",
    "    # One hot vector\n",
    "    vectorized = np.zeros((1,1,len(voc[1])))\n",
    "    vectorized[0,0,char2num[1][current_char]] = 1\n",
    "    # Decode\n",
    "    [dec_output, current_hidden, current_cell] = dec_model.predict([vectorized, current_hidden, current_cell], verbose=0)\n",
    "    # Best candidate index\n",
    "    best_candidate = np.argmax(dec_output[0,0,:])\n",
    "    # Update the current character\n",
    "    current_char = num2char[1][best_candidate]\n",
    "    dec_sentence+=current_char\n",
    "print(dec_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
